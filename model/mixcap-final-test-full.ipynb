{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "691b2bd4",
   "metadata": {
    "_cell_guid": "2694f487-766c-4c8a-bce8-baad42457ded",
    "_uuid": "99b61926-1a9c-4052-85ad-4eddb60a31c3",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.002837,
     "end_time": "2025-07-10T10:54:52.790552",
     "exception": false,
     "start_time": "2025-07-10T10:54:52.787715",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference + Evaluation Script (BLEU-4 & CIDEr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f33c611",
   "metadata": {
    "_cell_guid": "0ec92678-e83a-42da-9b72-37edc27a0c8b",
    "_uuid": "86bed9b1-0a5b-4504-9362-1d1d59a5997d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-10T10:54:52.796948Z",
     "iopub.status.busy": "2025-07-10T10:54:52.796109Z",
     "iopub.status.idle": "2025-07-10T10:55:11.342162Z",
     "shell.execute_reply": "2025-07-10T10:55:11.341372Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 18.550208,
     "end_time": "2025-07-10T10:55:11.343403",
     "exception": false,
     "start_time": "2025-07-10T10:54:52.793195",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'pycocoevalcap'...\r\n",
      "remote: Enumerating objects: 821, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (12/12), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (9/9), done.\u001b[K\r\n",
      "remote: Total 821 (delta 4), reused 3 (delta 3), pack-reused 809 (from 2)\u001b[K\r\n",
      "Receiving objects: 100% (821/821), 130.06 MiB | 35.79 MiB/s, done.\r\n",
      "Resolving deltas: 100% (424/424), done.\r\n",
      "/kaggle/working/pycocoevalcap\n",
      "Processing /kaggle/working/pycocoevalcap\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from pycocoevalcap==1.2) (2.0.10)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pycocotools>=2.0.2->pycocoevalcap==1.2) (1.26.4)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools>=2.0.2->pycocoevalcap==1.2) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools>=2.0.2->pycocoevalcap==1.2) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools>=2.0.2->pycocoevalcap==1.2) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools>=2.0.2->pycocoevalcap==1.2) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->pycocotools>=2.0.2->pycocoevalcap==1.2) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->pycocotools>=2.0.2->pycocoevalcap==1.2) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->pycocotools>=2.0.2->pycocoevalcap==1.2) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->pycocotools>=2.0.2->pycocoevalcap==1.2) (2024.2.0)\r\n",
      "Building wheels for collected packages: pycocoevalcap\r\n",
      "  Building wheel for pycocoevalcap (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for pycocoevalcap: filename=pycocoevalcap-1.2-py3-none-any.whl size=104312245 sha256=c27fa6535fa655a9ab2e41ffd67af4472f914f383249936b01970766d63c966b\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-3_8h83br/wheels/e1/95/5b/9a3357937c812a0ff04bc78701371bb96f914719385ff3183f\r\n",
      "Successfully built pycocoevalcap\r\n",
      "Installing collected packages: pycocoevalcap\r\n",
      "Successfully installed pycocoevalcap-1.2\r\n",
      "/kaggle/working\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/salaniz/pycocoevalcap.git\n",
    "%cd pycocoevalcap\n",
    "!pip install .\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff86684b",
   "metadata": {
    "_cell_guid": "0af579bf-16b4-4e11-9213-a8b1f71f0d72",
    "_uuid": "adb1813a-5ca9-4832-aeb0-fee8b7f52e3f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-10T10:55:11.351935Z",
     "iopub.status.busy": "2025-07-10T10:55:11.351689Z",
     "iopub.status.idle": "2025-07-10T10:55:16.587119Z",
     "shell.execute_reply": "2025-07-10T10:55:16.586506Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 5.241161,
     "end_time": "2025-07-10T10:55:16.588476",
     "exception": false,
     "start_time": "2025-07-10T10:55:11.347315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, csv, numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from pycocoevalcap.bleu.bleu import Bleu\n",
    "from pycocoevalcap.meteor.meteor import Meteor\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "# === CONFIG ===\n",
    "CHECKPOINT_PATH = \"/kaggle/input/mixcap-final-best-model/MixCap_model_only.pth\"\n",
    "SPM_PATH = \"/kaggle/input/mrsvtt-features-final-full-dataset/tokenizer/spm.model\"\n",
    "VAL_CAPTION_PATH = \"/kaggle/input/mrsvtt-features-final-full-dataset/tokenizer/tokenized_captions/test_captions.npy\"\n",
    "VIDEO_DIR = \"/kaggle/input/mrsvtt-features-final-full-dataset/test_set/video\"\n",
    "AUDIO_DIR = \"/kaggle/input/mrsvtt-features-final-full-dataset/test_set/audio\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load tokenizer\n",
    "sp_model = spm.SentencePieceProcessor()\n",
    "sp_model.load(SPM_PATH)\n",
    "\n",
    "PAD_ID = 4\n",
    "SOS_ID = 5\n",
    "EOS_ID = 6\n",
    "VOCAB_SIZE = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fa74795",
   "metadata": {
    "_cell_guid": "2e8ba785-0db5-4259-b184-d8725eee84ae",
    "_uuid": "b50e0c05-f4f8-4ddc-9da0-8bbe2186332e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-10T10:55:16.598037Z",
     "iopub.status.busy": "2025-07-10T10:55:16.597726Z",
     "iopub.status.idle": "2025-07-10T10:55:16.602764Z",
     "shell.execute_reply": "2025-07-10T10:55:16.602195Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.010335,
     "end_time": "2025-07-10T10:55:16.603849",
     "exception": false,
     "start_time": "2025-07-10T10:55:16.593514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EvalDataset(Dataset):\n",
    "    def __init__(self, video_dir, audio_dir, caption_dict):\n",
    "        self.video_dir = video_dir\n",
    "        self.audio_dir = audio_dir\n",
    "        self.caption_dict = caption_dict\n",
    "        self.vids = list(caption_dict.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vid = self.vids[idx]\n",
    "        video_np = np.load(os.path.join(self.video_dir, f\"{vid}_video.npy\"))\n",
    "        try:\n",
    "            audio_np = np.load(os.path.join(self.audio_dir, f\"{vid}_audio.npy\"))\n",
    "        except:\n",
    "            audio_np = np.zeros((1, 1024), dtype=np.float32)\n",
    "        return vid, video_np, audio_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1066bd17",
   "metadata": {
    "_cell_guid": "1229e50b-1f38-4a7e-a0cc-a508e0ded28c",
    "_uuid": "23d40456-ff9c-4114-b422-363cf63d3775",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-10T10:55:16.611461Z",
     "iopub.status.busy": "2025-07-10T10:55:16.611261Z",
     "iopub.status.idle": "2025-07-10T10:55:16.616536Z",
     "shell.execute_reply": "2025-07-10T10:55:16.616046Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.010223,
     "end_time": "2025-07-10T10:55:16.617515",
     "exception": false,
     "start_time": "2025-07-10T10:55:16.607292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_caption(model, video_np, audio_np, max_len=30):\n",
    "    model.eval()\n",
    "    video = torch.from_numpy(video_np).unsqueeze(0).float().to(DEVICE)\n",
    "    audio = torch.from_numpy(audio_np).unsqueeze(0).float().to(DEVICE)\n",
    "    v_mask = torch.zeros(1, video.size(1), dtype=torch.bool, device=DEVICE)\n",
    "    a_mask = torch.zeros(1, audio.size(1), dtype=torch.bool, device=DEVICE)\n",
    "\n",
    "    tgt = torch.tensor([[SOS_ID]], dtype=torch.long, device=DEVICE)\n",
    "    for _ in range(max_len):\n",
    "        logits = model(video, audio, tgt, v_mask, a_mask, tgt.eq(PAD_ID))\n",
    "        next_tok = logits[:, -1].argmax(-1, keepdim=True)\n",
    "        tgt = torch.cat([tgt, next_tok], dim=1)\n",
    "        if next_tok.item() == EOS_ID:\n",
    "            break\n",
    "    return tgt.squeeze().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3af4a8a7",
   "metadata": {
    "_cell_guid": "c867f6ba-22d5-4604-9c94-f0cdca227b24",
    "_uuid": "8d73d6e7-8c59-40e8-a39d-f6429e64bafb",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-10T10:55:16.625107Z",
     "iopub.status.busy": "2025-07-10T10:55:16.624913Z",
     "iopub.status.idle": "2025-07-10T10:55:16.638352Z",
     "shell.execute_reply": "2025-07-10T10:55:16.637607Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.018446,
     "end_time": "2025-07-10T10:55:16.639314",
     "exception": false,
     "start_time": "2025-07-10T10:55:16.620868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load model definition from training script\n",
    "# Make sure this matches exactly your architecture\n",
    "class PositionalEmbedding(torch.nn.Module):\n",
    "    def __init__(self, max_len:int, dim:int):\n",
    "        super().__init__()\n",
    "        self.embed = torch.nn.Embedding(max_len, dim)\n",
    "    def forward(self, x):\n",
    "        idx = torch.arange(x.size(1), device=x.device)\n",
    "        return x + self.embed(idx)[None, :, :]\n",
    "\n",
    "class CrossAttentionBlock(torch.nn.Module):\n",
    "    def __init__(self, dim:int, heads:int, dropout:float=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = torch.nn.MultiheadAttention(dim, heads, dropout, batch_first=True)\n",
    "        self.ff   = torch.nn.Sequential(torch.nn.Linear(dim, dim*4), torch.nn.ReLU(), torch.nn.Linear(dim*4, dim), torch.nn.Dropout(dropout))\n",
    "        self.norm1 = torch.nn.LayerNorm(dim); self.norm2 = torch.nn.LayerNorm(dim)\n",
    "    def forward(self, q, kv, kv_mask=None):\n",
    "        attn_out, _ = self.attn(q, kv, kv, key_padding_mask=kv_mask)\n",
    "        x = self.norm1(q + attn_out)\n",
    "        return self.norm2(x + self.ff(x))\n",
    "\n",
    "\n",
    "class MixcapEncoder(torch.nn.Module):\n",
    "    def __init__(self, v_dim=1408, a_dim=1024, f_dim=768, layers=4, heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.v_proj = torch.nn.Linear(v_dim, f_dim)\n",
    "        self.a_proj = torch.nn.Linear(a_dim, f_dim)\n",
    "        self.pe = PositionalEmbedding(320, f_dim)\n",
    "        self.drop = torch.nn.Dropout(dropout)\n",
    "        self.v2a = torch.nn.ModuleList([CrossAttentionBlock(f_dim, heads, dropout) for _ in range(layers)])\n",
    "        self.a2v = torch.nn.ModuleList([CrossAttentionBlock(f_dim, heads, dropout) for _ in range(layers)])\n",
    "\n",
    "    def forward(self, v, a, v_mask=None, a_mask=None):\n",
    "        v = self.drop(self.pe(self.v_proj(v)))\n",
    "        a = self.drop(self.pe(self.a_proj(a)))\n",
    "        for i in range(len(self.v2a)):\n",
    "            v = self.v2a[i](v, a, a_mask)\n",
    "            a = self.a2v[i](a, v, v_mask)\n",
    "        return v, a\n",
    "\n",
    "\n",
    "class CaptionDecoder(torch.nn.Module):\n",
    "    def __init__(self, f_dim=768, vocab=VOCAB_SIZE, layers=4, heads=8, ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed = torch.nn.Embedding(vocab, f_dim, padding_idx=PAD_ID)\n",
    "        self.pe = PositionalEmbedding(320, f_dim)\n",
    "        dec_layer = torch.nn.TransformerDecoderLayer(f_dim, heads, ff, dropout, batch_first=True)\n",
    "        self.trans = torch.nn.TransformerDecoder(dec_layer, layers)\n",
    "        self.out = torch.nn.Linear(f_dim, vocab)\n",
    "    def _causal_mask(self, T, device):\n",
    "        return torch.triu(torch.ones((T, T), dtype=torch.bool, device=device), 1)\n",
    "    def forward(self, tgt, memory, tgt_pad_mask=None, mem_pad_mask=None):\n",
    "        x = self.pe(self.embed(tgt))\n",
    "        causal = self._causal_mask(tgt.size(1), tgt.device)\n",
    "        return self.out(self.trans(x, memory,\n",
    "                                   tgt_mask=causal.masked_fill(causal, float('-inf')),\n",
    "                                   tgt_key_padding_mask=tgt_pad_mask,\n",
    "                                   memory_key_padding_mask=mem_pad_mask))\n",
    "\n",
    "class MixcapModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc = MixcapEncoder()\n",
    "        self.dec = CaptionDecoder()\n",
    "    def forward(self, v, a, tgt, v_mask=None, a_mask=None, tgt_pad_mask=None):\n",
    "        v_enc, a_enc = self.enc(v, a, v_mask, a_mask)\n",
    "        mem = torch.cat([v_enc, a_enc], dim=1)\n",
    "        mem_mask = torch.cat([v_mask, a_mask], dim=1) if v_mask is not None else None\n",
    "        return self.dec(tgt, mem, tgt_pad_mask, mem_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "961569f1",
   "metadata": {
    "_cell_guid": "ebb31504-72cf-48bb-bf5d-f9a01c2fee19",
    "_uuid": "d5f76abc-8740-4d64-81e5-a9813ad12d7d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-10T10:55:16.646835Z",
     "iopub.status.busy": "2025-07-10T10:55:16.646414Z",
     "iopub.status.idle": "2025-07-10T10:55:21.375216Z",
     "shell.execute_reply": "2025-07-10T10:55:21.374539Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 4.73368,
     "end_time": "2025-07-10T10:55:21.376327",
     "exception": false,
     "start_time": "2025-07-10T10:55:16.642647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixcapModel(\n",
       "  (enc): MixcapEncoder(\n",
       "    (v_proj): Linear(in_features=1408, out_features=768, bias=True)\n",
       "    (a_proj): Linear(in_features=1024, out_features=768, bias=True)\n",
       "    (pe): PositionalEmbedding(\n",
       "      (embed): Embedding(320, 768)\n",
       "    )\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (v2a): ModuleList(\n",
       "      (0-3): 4 x CrossAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ff): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (a2v): ModuleList(\n",
       "      (0-3): 4 x CrossAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ff): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dec): CaptionDecoder(\n",
       "    (embed): Embedding(8000, 768, padding_idx=4)\n",
       "    (pe): PositionalEmbedding(\n",
       "      (embed): Embedding(320, 768)\n",
       "    )\n",
       "    (trans): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (out): Linear(in_features=768, out_features=8000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model and checkpoint\n",
    "model = MixcapModel().to(DEVICE)\n",
    "ckpt = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
    "model.load_state_dict(ckpt)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb4f61ea",
   "metadata": {
    "_cell_guid": "d5fdc505-a65b-4d66-9347-cb825f1900a5",
    "_uuid": "252ca5e0-cd44-4837-a264-a78fc394dfc6",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-10T10:55:21.384753Z",
     "iopub.status.busy": "2025-07-10T10:55:21.384259Z",
     "iopub.status.idle": "2025-07-10T11:00:58.125424Z",
     "shell.execute_reply": "2025-07-10T11:00:58.124837Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 336.746676,
     "end_time": "2025-07-10T11:00:58.126795",
     "exception": false,
     "start_time": "2025-07-10T10:55:21.380119",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions and decoding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2990/2990 [05:10<00:00,  9.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'testlen': 22428, 'reflen': 22938, 'guess': [22428, 19438, 16448, 13458], 'correct': [19247, 11886, 6153, 2734]}\n",
      "ratio: 0.9777661522364208\n",
      "\n",
      "BLEU-1:  0.84\n",
      "BLEU-2:  0.71\n",
      "BLEU-3:  0.57\n",
      "BLEU-4:  0.44\n",
      "METEOR:  0.30\n",
      "ROUGE-L: 0.63\n",
      "CIDEr:   0.55\n",
      "\n",
      "\n",
      "Example Predictions:\n",
      "\n",
      "--- Example 1 ---\n",
      "Video ID: video9274\n",
      "Prediction: a cartoon character is playing with a ball\n",
      "References:\n",
      "  - a cartoon character speaks to the camera\n",
      "  - a cartoon si shown\n",
      "  - a clip from the tv show spongebob squarepants\n",
      "  - a recording of a tv showing spongebob square pants\n",
      "  - a scene from spongebob squarepants is shown\n",
      "  - a scene from spongebob squarepants\n",
      "  - an animated cartoon is looking at something and laughing\n",
      "  - animated cartoon scene spongebob\n",
      "  - cartoon character laugh at squid word then leave the restaurant\n",
      "  - cartoon character playing a clarinet\n",
      "\n",
      "--- Example 2 ---\n",
      "Video ID: video9251\n",
      "Prediction: a man is talking about a movie\n",
      "References:\n",
      "  - a ballet of sleeping beauty\n",
      "  - a group of people are dancing and a man talking\n",
      "  - a man is interviewed about theatre\n",
      "  - a man is sitting in a theatre talking about ballet\n",
      "  - a man talks about being succesful with several plays\n",
      "  - a scene from the musical sleeping beauty\n",
      "  - a scene previewing sleeping beauty\n",
      "  - a woman is dancing\n",
      "  - an interview with a choreographer\n",
      "  - clip of stage play\n",
      "\n",
      "--- Example 3 ---\n",
      "Video ID: video8306\n",
      "Prediction: a group of people are walking around outside\n",
      "References:\n",
      "  - there are some people walking in to the road\n",
      "  - there is a beauteous city some people are standing there\n",
      "  - the travelers are watching and taking photographs of the building\n",
      "  - someone is sharing their home video of a recent trip to mt\n",
      "  - scenery of and around mount fuji and the visitor center\n",
      "  - a lady talking something about some tourist place\n",
      "  - mount fuji visitors center parking lot with visitors taking photos and pictures\n",
      "  - a people are standing and walking in the road\n",
      "  - people walking and taking photos\n",
      "  - tourists walking around the mount fuji visitor center\n",
      "\n",
      "--- Example 4 ---\n",
      "Video ID: video9189\n",
      "Prediction: a girl is singing a song\n",
      "References:\n",
      "  - a group of four females are dancing to the music\n",
      "  - four girls in different kind of dresses singing and dancing in the room\n",
      "  - four girls standing and singing a song\n",
      "  - womens sing song and all dance togeher in a very common way\n",
      "  - four young girls singing together in harmony in a room\n",
      "  - four teenage girls singing in a video\n",
      "  - four teenage girls singing in a video\n",
      "  - the girls stand near the wall as they dance and sing\n",
      "  - the young girls are enjoying dancing and the songs in the hotel room\n",
      "  - a group of girls singing a song and dancing according to the rhythm\n",
      "\n",
      "--- Example 5 ---\n",
      "Video ID: video8288\n",
      "Prediction: a car is driving down the road\n",
      "References:\n",
      "  - its a blue car moving on a road\n",
      "  - there is a car moving normal speed without shake\n",
      "  - a blue car is moving on a road very fast\n",
      "  - there is a blue car moving very fast on the road\n",
      "  - a blue car is temporary driven by auto pilot with passenger in it\n",
      "  - temporary auto pilot car is going very fast\n",
      "  - a blue van driving fast on a grey road with white lines\n",
      "  - this is a blue car driving down the road\n",
      "  - a commercial shows a close-up of a car driving down the road\n",
      "  - veg and pepper salt and gravy and mixed new non-veg item prepare net\n",
      "\n",
      "--- Example 6 ---\n",
      "Video ID: video7291\n",
      "Prediction: a man is talking about the president\n",
      "References:\n",
      "  - a man is giving a speech\n",
      "  - a man is interviewing another man\n",
      "  - a man is speaking at a podium\n",
      "  - a man speaks to a camera for a video segment\n",
      "  - a man tells a joke at a podium\n",
      "  - a show with president obama and vice president biden\n",
      "  - a video showing clips from the jake vale show\n",
      "  - obama does an interview and a man gives a speech\n",
      "  - obama joking and a politician giving a speech\n",
      "  - obama talking to some guy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load captions and create dataset\n",
    "val_caps = np.load(VAL_CAPTION_PATH, allow_pickle=True).item()\n",
    "val_dataset = EvalDataset(VIDEO_DIR, AUDIO_DIR, val_caps)\n",
    "\n",
    "# Evaluate\n",
    "gts, res = {}, {}\n",
    "print(\"Generating predictions and decoding...\")\n",
    "\n",
    "for vid, video_np, audio_np in tqdm(val_dataset):\n",
    "    hyp_ids = generate_caption(model, video_np, audio_np)\n",
    "    hyp_text = sp_model.decode_ids(hyp_ids)\n",
    "    ref_texts = [sp_model.decode_ids(ref) for ref in val_caps[vid]]\n",
    "\n",
    "    gts[vid] = ref_texts\n",
    "    res[vid] = [hyp_text]\n",
    "\n",
    "# Compute metrics\n",
    "bleu = Bleu(4)\n",
    "cider = Cider()\n",
    "meteor = Meteor()\n",
    "rouge = Rouge()\n",
    "\n",
    "\n",
    "bleu_score, _ = bleu.compute_score(gts, res)\n",
    "cider_score, _ = cider.compute_score(gts, res)\n",
    "meteor_score, _ = meteor.compute_score(gts, res)\n",
    "rouge_score, _  = rouge.compute_score(gts, res)\n",
    "\n",
    "\n",
    "# print(f\"\\n BLEU-4: {bleu_score[3]:.2f} | CIDEr: {cider_score:.2f}\")\n",
    "print(f\"\"\"\n",
    "BLEU-1:  {bleu_score[0]:.2f}\n",
    "BLEU-2:  {bleu_score[1]:.2f}\n",
    "BLEU-3:  {bleu_score[2]:.2f}\n",
    "BLEU-4:  {bleu_score[3]:.2f}\n",
    "METEOR:  {meteor_score:.2f}\n",
    "ROUGE-L: {rouge_score:.2f}\n",
    "CIDEr:   {cider_score:.2f}\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "# === Print 6 examples ===\n",
    "print(\"\\nExample Predictions:\\n\")\n",
    "\n",
    "example_vids = random.sample(list(gts.keys()), 6)\n",
    "for i, vid in enumerate(example_vids, 1):\n",
    "    print(f\"--- Example {i} ---\")\n",
    "    print(f\"Video ID: {vid}\")\n",
    "    print(f\"Prediction: {res[vid][0]}\")\n",
    "    print(f\"References:\")\n",
    "    for ref in gts[vid][:10]: \n",
    "        print(f\"  - {ref}\")\n",
    "    print()\n",
    "\n",
    "import pandas as pd\n",
    "records = [{\"video_id\": vid, \"prediction\": res[vid][0], \"references\": \" ||| \".join(gts[vid])} for vid in gts]\n",
    "df = pd.DataFrame(records)\n",
    "df.to_csv(\"evaluation_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7525347,
     "sourceId": 11967559,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 249645929,
     "sourceType": "kernelVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 372.643478,
   "end_time": "2025-07-10T11:01:00.917901",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-10T10:54:48.274423",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
