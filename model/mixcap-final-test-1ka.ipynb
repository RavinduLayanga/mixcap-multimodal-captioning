{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a017c40",
   "metadata": {
    "_cell_guid": "842625c3-484c-45ae-9fe3-71c13847821b",
    "_uuid": "3968add6-11ad-4bc2-8062-d1a31e72f1eb",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.003156,
     "end_time": "2025-07-10T11:28:35.551749",
     "exception": false,
     "start_time": "2025-07-10T11:28:35.548593",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference + Evaluation Script (BLEU-4 & CIDEr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16d254c2",
   "metadata": {
    "_cell_guid": "2c233de0-6cf9-474a-8861-0649d4ddf177",
    "_uuid": "529251ef-c4c2-4053-9597-84f70670db5e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-10T11:28:35.558678Z",
     "iopub.status.busy": "2025-07-10T11:28:35.558383Z",
     "iopub.status.idle": "2025-07-10T11:28:56.216899Z",
     "shell.execute_reply": "2025-07-10T11:28:56.215747Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 20.663602,
     "end_time": "2025-07-10T11:28:56.218554",
     "exception": false,
     "start_time": "2025-07-10T11:28:35.554952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'pycocoevalcap'...\r\n",
      "remote: Enumerating objects: 821, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (12/12), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (9/9), done.\u001b[K\r\n",
      "remote: Total 821 (delta 4), reused 3 (delta 3), pack-reused 809 (from 2)\u001b[K\r\n",
      "Receiving objects: 100% (821/821), 130.06 MiB | 39.39 MiB/s, done.\r\n",
      "Resolving deltas: 100% (424/424), done.\r\n",
      "/kaggle/working/pycocoevalcap\n",
      "Processing /kaggle/working/pycocoevalcap\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from pycocoevalcap==1.2) (2.0.10)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pycocotools>=2.0.2->pycocoevalcap==1.2) (1.26.4)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools>=2.0.2->pycocoevalcap==1.2) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools>=2.0.2->pycocoevalcap==1.2) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools>=2.0.2->pycocoevalcap==1.2) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools>=2.0.2->pycocoevalcap==1.2) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->pycocotools>=2.0.2->pycocoevalcap==1.2) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->pycocotools>=2.0.2->pycocoevalcap==1.2) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->pycocotools>=2.0.2->pycocoevalcap==1.2) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->pycocotools>=2.0.2->pycocoevalcap==1.2) (2024.2.0)\r\n",
      "Building wheels for collected packages: pycocoevalcap\r\n",
      "  Building wheel for pycocoevalcap (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for pycocoevalcap: filename=pycocoevalcap-1.2-py3-none-any.whl size=104312245 sha256=cc9cde300bab384ec9282b5ba3104d3a0e6ced2abff415fba77d679502105c3c\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-njmoxolm/wheels/e1/95/5b/9a3357937c812a0ff04bc78701371bb96f914719385ff3183f\r\n",
      "Successfully built pycocoevalcap\r\n",
      "Installing collected packages: pycocoevalcap\r\n",
      "Successfully installed pycocoevalcap-1.2\r\n",
      "/kaggle/working\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/salaniz/pycocoevalcap.git\n",
    "%cd pycocoevalcap\n",
    "!pip install .\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4abfcecc",
   "metadata": {
    "_cell_guid": "d49e4a0e-a87f-4548-a4f8-a47b88f430b0",
    "_uuid": "4b421bf0-b902-456a-9fdf-8d89bf5bd1db",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-10T11:28:56.229171Z",
     "iopub.status.busy": "2025-07-10T11:28:56.228823Z",
     "iopub.status.idle": "2025-07-10T11:29:02.276792Z",
     "shell.execute_reply": "2025-07-10T11:29:02.275981Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 6.055059,
     "end_time": "2025-07-10T11:29:02.278506",
     "exception": false,
     "start_time": "2025-07-10T11:28:56.223447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, csv, numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from pycocoevalcap.bleu.bleu import Bleu\n",
    "from pycocoevalcap.meteor.meteor import Meteor\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "# === CONFIG ===\n",
    "CHECKPOINT_PATH = \"/kaggle/input/mixcap-final-best-model/MixCap_model_only.pth\"\n",
    "SPM_PATH = \"/kaggle/input/mrsvtt-features-final-full-dataset/tokenizer/spm.model\"\n",
    "VAL_CAPTION_PATH = \"/kaggle/input/mrsvtt-features-final-full-dataset/tokenizer/tokenized_captions/1kA_captions.npy\"\n",
    "VIDEO_DIR = \"/kaggle/input/mrsvtt-features-final-full-dataset/test_set/video\"\n",
    "AUDIO_DIR = \"/kaggle/input/mrsvtt-features-final-full-dataset/test_set/audio\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load tokenizer\n",
    "sp_model = spm.SentencePieceProcessor()\n",
    "sp_model.load(SPM_PATH)\n",
    "\n",
    "PAD_ID = 4\n",
    "SOS_ID = 5\n",
    "EOS_ID = 6\n",
    "VOCAB_SIZE = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ea8b5d8",
   "metadata": {
    "_cell_guid": "d5775acd-28f3-4c81-8d6b-03e754d47868",
    "_uuid": "0c7d3e53-87bb-4980-948c-7e541bfcefff",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-10T11:29:02.288506Z",
     "iopub.status.busy": "2025-07-10T11:29:02.288130Z",
     "iopub.status.idle": "2025-07-10T11:29:02.294176Z",
     "shell.execute_reply": "2025-07-10T11:29:02.293461Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.012515,
     "end_time": "2025-07-10T11:29:02.295473",
     "exception": false,
     "start_time": "2025-07-10T11:29:02.282958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EvalDataset(Dataset):\n",
    "    def __init__(self, video_dir, audio_dir, caption_dict):\n",
    "        self.video_dir = video_dir\n",
    "        self.audio_dir = audio_dir\n",
    "        self.caption_dict = caption_dict\n",
    "        self.vids = list(caption_dict.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vid = self.vids[idx]\n",
    "        video_np = np.load(os.path.join(self.video_dir, f\"{vid}_video.npy\"))\n",
    "        try:\n",
    "            audio_np = np.load(os.path.join(self.audio_dir, f\"{vid}_audio.npy\"))\n",
    "        except:\n",
    "            audio_np = np.zeros((1, 1024), dtype=np.float32)\n",
    "        return vid, video_np, audio_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64e83cfa",
   "metadata": {
    "_cell_guid": "5e5e431e-fc03-42ce-abf7-fa676e2f1a30",
    "_uuid": "9a702b36-6e7b-462f-8a2e-63e484c23d71",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-10T11:29:02.305838Z",
     "iopub.status.busy": "2025-07-10T11:29:02.305545Z",
     "iopub.status.idle": "2025-07-10T11:29:02.311832Z",
     "shell.execute_reply": "2025-07-10T11:29:02.311200Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.01322,
     "end_time": "2025-07-10T11:29:02.313019",
     "exception": false,
     "start_time": "2025-07-10T11:29:02.299799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_caption(model, video_np, audio_np, max_len=30):\n",
    "    model.eval()\n",
    "    video = torch.from_numpy(video_np).unsqueeze(0).float().to(DEVICE)\n",
    "    audio = torch.from_numpy(audio_np).unsqueeze(0).float().to(DEVICE)\n",
    "    v_mask = torch.zeros(1, video.size(1), dtype=torch.bool, device=DEVICE)\n",
    "    a_mask = torch.zeros(1, audio.size(1), dtype=torch.bool, device=DEVICE)\n",
    "\n",
    "    tgt = torch.tensor([[SOS_ID]], dtype=torch.long, device=DEVICE)\n",
    "    for _ in range(max_len):\n",
    "        logits = model(video, audio, tgt, v_mask, a_mask, tgt.eq(PAD_ID))\n",
    "        next_tok = logits[:, -1].argmax(-1, keepdim=True)\n",
    "        tgt = torch.cat([tgt, next_tok], dim=1)\n",
    "        if next_tok.item() == EOS_ID:\n",
    "            break\n",
    "    return tgt.squeeze().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c065524",
   "metadata": {
    "_cell_guid": "cc2f83fe-fa58-4a4d-84a0-20597a5c546c",
    "_uuid": "89806f4c-ba6b-4d31-bc56-77c2a420a1ed",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-10T11:29:02.322714Z",
     "iopub.status.busy": "2025-07-10T11:29:02.322420Z",
     "iopub.status.idle": "2025-07-10T11:29:02.338503Z",
     "shell.execute_reply": "2025-07-10T11:29:02.337755Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.022618,
     "end_time": "2025-07-10T11:29:02.339835",
     "exception": false,
     "start_time": "2025-07-10T11:29:02.317217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load model definition from training script\n",
    "# Make sure this matches exactly your architecture\n",
    "class PositionalEmbedding(torch.nn.Module):\n",
    "    def __init__(self, max_len:int, dim:int):\n",
    "        super().__init__()\n",
    "        self.embed = torch.nn.Embedding(max_len, dim)\n",
    "    def forward(self, x):\n",
    "        idx = torch.arange(x.size(1), device=x.device)\n",
    "        return x + self.embed(idx)[None, :, :]\n",
    "\n",
    "class CrossAttentionBlock(torch.nn.Module):\n",
    "    def __init__(self, dim:int, heads:int, dropout:float=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = torch.nn.MultiheadAttention(dim, heads, dropout, batch_first=True)\n",
    "        self.ff   = torch.nn.Sequential(torch.nn.Linear(dim, dim*4), torch.nn.ReLU(), torch.nn.Linear(dim*4, dim), torch.nn.Dropout(dropout))\n",
    "        self.norm1 = torch.nn.LayerNorm(dim); self.norm2 = torch.nn.LayerNorm(dim)\n",
    "    def forward(self, q, kv, kv_mask=None):\n",
    "        attn_out, _ = self.attn(q, kv, kv, key_padding_mask=kv_mask)\n",
    "        x = self.norm1(q + attn_out)\n",
    "        return self.norm2(x + self.ff(x))\n",
    "\n",
    "\n",
    "class MixcapEncoder(torch.nn.Module):\n",
    "    def __init__(self, v_dim=1408, a_dim=1024, f_dim=768, layers=4, heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.v_proj = torch.nn.Linear(v_dim, f_dim)\n",
    "        self.a_proj = torch.nn.Linear(a_dim, f_dim)\n",
    "        self.pe = PositionalEmbedding(320, f_dim)\n",
    "        self.drop = torch.nn.Dropout(dropout)\n",
    "        self.v2a = torch.nn.ModuleList([CrossAttentionBlock(f_dim, heads, dropout) for _ in range(layers)])\n",
    "        self.a2v = torch.nn.ModuleList([CrossAttentionBlock(f_dim, heads, dropout) for _ in range(layers)])\n",
    "\n",
    "    def forward(self, v, a, v_mask=None, a_mask=None):\n",
    "        v = self.drop(self.pe(self.v_proj(v)))\n",
    "        a = self.drop(self.pe(self.a_proj(a)))\n",
    "        for i in range(len(self.v2a)):\n",
    "            v = self.v2a[i](v, a, a_mask)\n",
    "            a = self.a2v[i](a, v, v_mask)\n",
    "        return v, a\n",
    "\n",
    "\n",
    "class CaptionDecoder(torch.nn.Module):\n",
    "    def __init__(self, f_dim=768, vocab=VOCAB_SIZE, layers=4, heads=8, ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed = torch.nn.Embedding(vocab, f_dim, padding_idx=PAD_ID)\n",
    "        self.pe = PositionalEmbedding(320, f_dim)\n",
    "        dec_layer = torch.nn.TransformerDecoderLayer(f_dim, heads, ff, dropout, batch_first=True)\n",
    "        self.trans = torch.nn.TransformerDecoder(dec_layer, layers)\n",
    "        self.out = torch.nn.Linear(f_dim, vocab)\n",
    "    def _causal_mask(self, T, device):\n",
    "        return torch.triu(torch.ones((T, T), dtype=torch.bool, device=device), 1)\n",
    "    def forward(self, tgt, memory, tgt_pad_mask=None, mem_pad_mask=None):\n",
    "        x = self.pe(self.embed(tgt))\n",
    "        causal = self._causal_mask(tgt.size(1), tgt.device)\n",
    "        return self.out(self.trans(x, memory,\n",
    "                                   tgt_mask=causal.masked_fill(causal, float('-inf')),\n",
    "                                   tgt_key_padding_mask=tgt_pad_mask,\n",
    "                                   memory_key_padding_mask=mem_pad_mask))\n",
    "\n",
    "class MixcapModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.enc = MixcapEncoder()\n",
    "        self.dec = CaptionDecoder()\n",
    "    def forward(self, v, a, tgt, v_mask=None, a_mask=None, tgt_pad_mask=None):\n",
    "        v_enc, a_enc = self.enc(v, a, v_mask, a_mask)\n",
    "        mem = torch.cat([v_enc, a_enc], dim=1)\n",
    "        mem_mask = torch.cat([v_mask, a_mask], dim=1) if v_mask is not None else None\n",
    "        return self.dec(tgt, mem, tgt_pad_mask, mem_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eb8e960",
   "metadata": {
    "_cell_guid": "62ad7a75-a287-4220-b669-515c90b79b73",
    "_uuid": "ec518dd2-1d50-4bcf-88df-95f9f2d591df",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-10T11:29:02.349743Z",
     "iopub.status.busy": "2025-07-10T11:29:02.349446Z",
     "iopub.status.idle": "2025-07-10T11:29:07.739278Z",
     "shell.execute_reply": "2025-07-10T11:29:07.738354Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 5.396418,
     "end_time": "2025-07-10T11:29:07.740735",
     "exception": false,
     "start_time": "2025-07-10T11:29:02.344317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixcapModel(\n",
       "  (enc): MixcapEncoder(\n",
       "    (v_proj): Linear(in_features=1408, out_features=768, bias=True)\n",
       "    (a_proj): Linear(in_features=1024, out_features=768, bias=True)\n",
       "    (pe): PositionalEmbedding(\n",
       "      (embed): Embedding(320, 768)\n",
       "    )\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (v2a): ModuleList(\n",
       "      (0-3): 4 x CrossAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ff): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (a2v): ModuleList(\n",
       "      (0-3): 4 x CrossAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ff): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dec): CaptionDecoder(\n",
       "    (embed): Embedding(8000, 768, padding_idx=4)\n",
       "    (pe): PositionalEmbedding(\n",
       "      (embed): Embedding(320, 768)\n",
       "    )\n",
       "    (trans): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (out): Linear(in_features=768, out_features=8000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model and checkpoint\n",
    "model = MixcapModel().to(DEVICE)\n",
    "ckpt = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
    "model.load_state_dict(ckpt)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d0b21f1",
   "metadata": {
    "_cell_guid": "a6e4d8dd-ed6d-45ee-a156-65f31a363be7",
    "_uuid": "c5f04dc5-bc7d-4629-9a14-1c7a2d200b5a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-07-10T11:29:07.751709Z",
     "iopub.status.busy": "2025-07-10T11:29:07.751420Z",
     "iopub.status.idle": "2025-07-10T11:39:15.889309Z",
     "shell.execute_reply": "2025-07-10T11:39:15.888553Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 608.14541,
     "end_time": "2025-07-10T11:39:15.891059",
     "exception": false,
     "start_time": "2025-07-10T11:29:07.745649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions and decoding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [09:47<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'testlen': 7556, 'reflen': 7720, 'guess': [7556, 6556, 5556, 4556], 'correct': [6476, 4026, 2050, 868]}\n",
      "ratio: 0.9787564766838109\n",
      "\n",
      "BLEU-1:  0.84\n",
      "BLEU-2:  0.71\n",
      "BLEU-3:  0.57\n",
      "BLEU-4:  0.43\n",
      "METEOR:  0.29\n",
      "ROUGE-L: 0.62\n",
      "CIDEr:   0.53\n",
      "\n",
      "\n",
      "Example Predictions:\n",
      "\n",
      "--- Example 1 ---\n",
      "Video ID: video9825\n",
      "Prediction: a man in a suit is talking about the psychology of the human body\n",
      "References:\n",
      "  - an intelligent man with glasses talk about certain phrenologists\n",
      "  - a man with black glasses talks about the first phrenologist\n",
      "  - a man in glasses discusses the first phrenologist and the practice of phrenology\n",
      "  - a man with eyeglasses is talking about a personality related to phrenology\n",
      "  - the man with spectacles and brown coat is talking by sitting on the chair\n",
      "  - a man in a brown suit coat in a black chair discussing phrenology the study of skull structure to determine personality\n",
      "  - a young man in glasses talks about phrenology\n",
      "  - a man in nerdy glasses and a beige suit jacket talks about phrenology the science of analyzing bone structure of the skull and drawing conclusions about one s character\n",
      "  - a man with brown suit explains about phrenology\n",
      "  - a man in glasses with blonde hair sitting in a chair\n",
      "\n",
      "--- Example 2 ---\n",
      "Video ID: video9577\n",
      "Prediction: a girl is riding a horse in a video game\n",
      "References:\n",
      "  - a small girl is sitting on a horse and then gets to the ground\n",
      "  - its a animated horse and a female\n",
      "  - there is kid with her horse and watching\n",
      "  - it is he game that where the lady is explaining the about the game and the functioning of the horse\n",
      "  - the narrator discusses her character as part of an online game\n",
      "  - an animated cartoon is shown for the childrens on the screen a girl ridding a horse\n",
      "  - an animation of a cartoon girl with a brown horse\n",
      "  - a girl riding a horse and wearing blue shirt steps down from horse\n",
      "  - the video is from a famous cartoon programme\n",
      "  - cartoon one women in horse and speak to that calmly\n",
      "\n",
      "--- Example 3 ---\n",
      "Video ID: video8915\n",
      "Prediction: a woman is making food\n",
      "References:\n",
      "  - a person is cooking\n",
      "  - a woman in the kitchen rolling some treats in cinnamon\n",
      "  - a woman is cooking food\n",
      "  - a woman is demonstrating how to make cinnamon twists\n",
      "  - a woman is mixing some food with some flour and talking about that\n",
      "  - a woman is preparing and serving food in a kitchen\n",
      "  - a woman is preparing food in a kitchen\n",
      "  - a woman making cinnamon twists\n",
      "  - a woman puts sugar on churros and plates them\n",
      "  - a woman rolling some type of food in a powder substance cooking related\n",
      "\n",
      "--- Example 4 ---\n",
      "Video ID: video8253\n",
      "Prediction: a basketball player scores a goal\n",
      "References:\n",
      "  - a basketball game scenes with music playing with it\n",
      "  - a man attempts to slam dunk a basketball\n",
      "  - a man is dribbling is basketball\n",
      "  - a musical montage featuring sports highlights\n",
      "  - a vine shows basketball clips\n",
      "  - basketball highlights with great songs\n",
      "  - compilation of popular vines\n",
      "  - hakeem fakes out his opponent with the dream shake\n",
      "  - highlight reel from basketball is playing\n",
      "  - highlights of a basketball plays between multiple players\n",
      "\n",
      "--- Example 5 ---\n",
      "Video ID: video8622\n",
      "Prediction: a person is solving a piece of paper\n",
      "References:\n",
      "  - a crowd gathers around a rubiks cube\n",
      "  - a group of people are looking at a rubik s cube on a table\n",
      "  - a group of people standing around a table\n",
      "  - a man completed a rubix cube\n",
      "  - a man has tried to solve a rubik s cube\n",
      "  - a man is looking at a rubicks cube\n",
      "  - a person attempting to solve a rubik s cube\n",
      "  - a person completes a rubix cube\n",
      "  - a person is doing a rubic s cube\n",
      "  - a team of people are looking at a rubick s cube\n",
      "\n",
      "--- Example 6 ---\n",
      "Video ID: video9691\n",
      "Prediction: a car is being displayed\n",
      "References:\n",
      "  - a car drives around at night\n",
      "  - a car is being driven\n",
      "  - a car is driven on a simulated track to show off its interior\n",
      "  - a commercial for the mazda 3 the card sliding around a corner\n",
      "  - a man demonstrates driving a car as an advertisement\n",
      "  - a man drives a car\n",
      "  - a man drives a mazda on a track\n",
      "  - a man driving a car\n",
      "  - a man zooms around in a new car commercial\n",
      "  - a red car drives on the slick road\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load captions and create dataset\n",
    "val_caps = np.load(VAL_CAPTION_PATH, allow_pickle=True).item()\n",
    "val_dataset = EvalDataset(VIDEO_DIR, AUDIO_DIR, val_caps)\n",
    "\n",
    "# Evaluate\n",
    "gts, res = {}, {}\n",
    "print(\"Generating predictions and decoding...\")\n",
    "\n",
    "for vid, video_np, audio_np in tqdm(val_dataset):\n",
    "    hyp_ids = generate_caption(model, video_np, audio_np)\n",
    "    hyp_text = sp_model.decode_ids(hyp_ids)\n",
    "    ref_texts = [sp_model.decode_ids(ref) for ref in val_caps[vid]]\n",
    "\n",
    "    gts[vid] = ref_texts\n",
    "    res[vid] = [hyp_text]\n",
    "\n",
    "# Compute metrics\n",
    "bleu = Bleu(4)\n",
    "cider = Cider()\n",
    "meteor = Meteor()\n",
    "rouge = Rouge()\n",
    "\n",
    "\n",
    "bleu_score, _ = bleu.compute_score(gts, res)\n",
    "cider_score, _ = cider.compute_score(gts, res)\n",
    "meteor_score, _ = meteor.compute_score(gts, res)\n",
    "rouge_score, _  = rouge.compute_score(gts, res)\n",
    "\n",
    "\n",
    "# print(f\"\\n BLEU-4: {bleu_score[3]:.2f} | CIDEr: {cider_score:.2f}\")\n",
    "print(f\"\"\"\n",
    "BLEU-1:  {bleu_score[0]:.2f}\n",
    "BLEU-2:  {bleu_score[1]:.2f}\n",
    "BLEU-3:  {bleu_score[2]:.2f}\n",
    "BLEU-4:  {bleu_score[3]:.2f}\n",
    "METEOR:  {meteor_score:.2f}\n",
    "ROUGE-L: {rouge_score:.2f}\n",
    "CIDEr:   {cider_score:.2f}\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "# === Print 6 examples ===\n",
    "print(\"\\nExample Predictions:\\n\")\n",
    "\n",
    "example_vids = random.sample(list(gts.keys()), 6)\n",
    "for i, vid in enumerate(example_vids, 1):\n",
    "    print(f\"--- Example {i} ---\")\n",
    "    print(f\"Video ID: {vid}\")\n",
    "    print(f\"Prediction: {res[vid][0]}\")\n",
    "    print(f\"References:\")\n",
    "    for ref in gts[vid][:10]: \n",
    "        print(f\"  - {ref}\")\n",
    "    print()\n",
    "\n",
    "import pandas as pd\n",
    "records = [{\"video_id\": vid, \"prediction\": res[vid][0], \"references\": \" ||| \".join(gts[vid])} for vid in gts]\n",
    "df = pd.DataFrame(records)\n",
    "df.to_csv(\"evaluation_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7525347,
     "sourceId": 11967559,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 249645929,
     "sourceType": "kernelVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 648.252624,
   "end_time": "2025-07-10T11:39:18.565132",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-10T11:28:30.312508",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
